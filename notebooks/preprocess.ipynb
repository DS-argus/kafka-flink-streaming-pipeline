{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `events.csv` 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 데이터 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 필요한 라이브러리 임포트 및 SparkSession 설정\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "spark = SparkSession.builder.appName(\"DE_project\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Events 데이터 스키마:\n",
            "root\n",
            " |-- display_id: integer (nullable = true)\n",
            " |-- uuid: string (nullable = true)\n",
            " |-- document_id: integer (nullable = true)\n",
            " |-- timestamp: integer (nullable = true)\n",
            " |-- platform: string (nullable = true)\n",
            " |-- geo_location: string (nullable = true)\n",
            "\n",
            "\n",
            "샘플 데이터:\n",
            "+----------+--------------+-----------+---------+--------+------------+\n",
            "|display_id|          uuid|document_id|timestamp|platform|geo_location|\n",
            "+----------+--------------+-----------+---------+--------+------------+\n",
            "|         1|cb8c55702adb93|     379743|       61|       3|   US>SC>519|\n",
            "|         2|79a85fa78311b9|    1794259|       81|       2|   US>CA>807|\n",
            "|         3|822932ce3d8757|    1179111|      182|       2|   US>MI>505|\n",
            "|         4|85281d0a49f7ac|    1777797|      234|       2|   US>WV>564|\n",
            "|         5|8d0daef4bf5b56|     252458|      338|       2|       SG>00|\n",
            "+----------+--------------+-----------+---------+--------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# events 데이터 읽기 및 구조 확인\n",
        "df_events = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"../rawdata/events.csv\")\n",
        "\n",
        "print(\"Events 데이터 스키마:\")\n",
        "df_events.printSchema()\n",
        "print(\"\\n샘플 데이터:\")\n",
        "df_events.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### partition 기준으로 사용할 event_date 컬럼 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "처리된 데이터 샘플:\n",
            "+---------+--------------------+----------+--------+\n",
            "|timestamp|          event_time|event_date|platform|\n",
            "+---------+--------------------+----------+--------+\n",
            "|       61|2016-06-14 04:00:...|2016-06-14|       3|\n",
            "|       81|2016-06-14 04:00:...|2016-06-14|       2|\n",
            "|      182|2016-06-14 04:00:...|2016-06-14|       2|\n",
            "|      234|2016-06-14 04:00:...|2016-06-14|       2|\n",
            "|      338|2016-06-14 04:00:...|2016-06-14|       2|\n",
            "+---------+--------------------+----------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# timestamp를 기반으로 event_time과 event_date 컬럼 생성\n",
        "df_events_processed = (df_events\n",
        "    .withColumn(\"event_time\", ((F.col(\"timestamp\") + 1465876799998)/1000).cast(\"timestamp\"))\n",
        "    .withColumn(\"event_date\", F.to_date(\"event_time\"))\n",
        ")\n",
        "\n",
        "# 샘플로 처리된 데이터 확인\n",
        "print(\"처리된 데이터 샘플:\")\n",
        "df_events_processed.select(\"timestamp\", \"event_time\", \"event_date\", \"platform\").show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### platform이 누락된 레코드 확인 및 입력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Platform이 1, 2, 3이 아닌 데이터:\n",
            "+----------+--------------+-----------+---------+--------+------------+--------------------+----------+\n",
            "|display_id|          uuid|document_id|timestamp|platform|geo_location|          event_time|event_date|\n",
            "+----------+--------------+-----------+---------+--------+------------+--------------------+----------+\n",
            "|    303066|83e9ec48908c6a|     968149| 28799999|      \\N|   US>VA>511|2016-06-14 11:59:...|2016-06-14|\n",
            "|  11328496|7e8aa06b36db6a|    1827718|752399961|      \\N|   US>KY>529|2016-06-22 20:59:...|2016-06-22|\n",
            "|  13489553|5cda9845a1b9be|    2624774|896400000|      \\N|   US>IN>527|2016-06-24 12:59:...|2016-06-24|\n",
            "|  14004328|f4bb634c3871b9|     635051|921599944|      \\N|   US>NY>501|2016-06-24 19:59:...|2016-06-24|\n",
            "|  15056922|558ba104e8a37c|    2746860|997199999|      \\N|   US>MA>506|2016-06-25 16:59:...|2016-06-25|\n",
            "+----------+--------------+-----------+---------+--------+------------+--------------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"🔍 Platform이 1, 2, 3이 아닌 데이터:\")\n",
        "df_events_processed.filter(~F.col(\"platform\").isin(['1', '2', '3'])).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# platform missing value 처리 based on page_views.csv\n",
        "# +----------+--------------+-----------+---------+--------+------------+\n",
        "# |display_id|          uuid|document_id|timestamp|platform|geo_location|\n",
        "# +----------+--------------+-----------+---------+--------+------------+\n",
        "# |    303066|83e9ec48908c6a|     968149| 28799999|      \\N|   US>VA>511| -> platform 2\n",
        "# |  11328496|7e8aa06b36db6a|    1827718|752399961|      \\N|   US>KY>529| -> platform 1\n",
        "# |  13489553|5cda9845a1b9be|    2624774|896400000|      \\N|   US>IN>527| -> platform 3\n",
        "# |  14004328|f4bb634c3871b9|     635051|921599944|      \\N|   US>NY>501| -> platform 1\n",
        "# |  15056922|558ba104e8a37c|    2746860|997199999|      \\N|   US>MA>506| -> platform 1\n",
        "# +----------+--------------+-----------+---------+--------+------------+\n",
        "\n",
        "df_events_processed = df_events_processed.withColumn(\"platform\",\n",
        "    F.when(F.col(\"display_id\").isin([11328496, 14004328, 15056922]), \"1\")\n",
        "    .when(F.col(\"display_id\").isin([303066]), \"2\") \n",
        "    .when(F.col(\"display_id\").isin([13489553]), \"3\")\n",
        "    .otherwise(F.col(\"platform\"))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+--------------+-----------+---------+--------+------------+--------------------+----------+\n",
            "|display_id|          uuid|document_id|timestamp|platform|geo_location|          event_time|event_date|\n",
            "+----------+--------------+-----------+---------+--------+------------+--------------------+----------+\n",
            "|    303066|83e9ec48908c6a|     968149| 28799999|       2|   US>VA>511|2016-06-14 11:59:...|2016-06-14|\n",
            "|  11328496|7e8aa06b36db6a|    1827718|752399961|       1|   US>KY>529|2016-06-22 20:59:...|2016-06-22|\n",
            "|  13489553|5cda9845a1b9be|    2624774|896400000|       3|   US>IN>527|2016-06-24 12:59:...|2016-06-24|\n",
            "|  14004328|f4bb634c3871b9|     635051|921599944|       1|   US>NY>501|2016-06-24 19:59:...|2016-06-24|\n",
            "|  15056922|558ba104e8a37c|    2746860|997199999|       1|   US>MA>506|2016-06-25 16:59:...|2016-06-25|\n",
            "+----------+--------------+-----------+---------+--------+------------+--------------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 확인\n",
        "df_events_processed.filter(F.col(\"display_id\").isin([303066, 11328496, 13489553, 14004328, 15056922])).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### platform - event_date 구조로 나누고 timestamp로 정렬해서 parquet으로 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "총 48개의 파티션을 개별 정렬해서 저장합니다...\n",
            "처리 중: platform=3, event_date=2016-06-15\n",
            "처리 중: platform=1, event_date=2016-06-14\n",
            "처리 중: platform=2, event_date=2016-06-15\n",
            "처리 중: platform=2, event_date=2016-06-14\n",
            "처리 중: platform=3, event_date=2016-06-14\n",
            "처리 중: platform=1, event_date=2016-06-15\n",
            "처리 중: platform=2, event_date=2016-06-16\n",
            "처리 중: platform=3, event_date=2016-06-17\n",
            "처리 중: platform=1, event_date=2016-06-17\n",
            "처리 중: platform=2, event_date=2016-06-17\n",
            "처리 중: platform=1, event_date=2016-06-16\n",
            "처리 중: platform=3, event_date=2016-06-16\n",
            "처리 중: platform=3, event_date=2016-06-18\n",
            "처리 중: platform=2, event_date=2016-06-19\n",
            "처리 중: platform=3, event_date=2016-06-19\n",
            "처리 중: platform=1, event_date=2016-06-19\n",
            "처리 중: platform=2, event_date=2016-06-18\n",
            "처리 중: platform=1, event_date=2016-06-18\n",
            "처리 중: platform=3, event_date=2016-06-20\n",
            "처리 중: platform=3, event_date=2016-06-21\n",
            "처리 중: platform=2, event_date=2016-06-21\n",
            "처리 중: platform=2, event_date=2016-06-20\n",
            "처리 중: platform=1, event_date=2016-06-20\n",
            "처리 중: platform=1, event_date=2016-06-21\n",
            "처리 중: platform=1, event_date=2016-06-23\n",
            "처리 중: platform=3, event_date=2016-06-22\n",
            "처리 중: platform=1, event_date=2016-06-22\n",
            "처리 중: platform=3, event_date=2016-06-23\n",
            "처리 중: platform=2, event_date=2016-06-22\n",
            "처리 중: platform=2, event_date=2016-06-23\n",
            "처리 중: platform=1, event_date=2016-06-24\n",
            "처리 중: platform=2, event_date=2016-06-24\n",
            "처리 중: platform=3, event_date=2016-06-24\n",
            "처리 중: platform=2, event_date=2016-06-26\n",
            "처리 중: platform=1, event_date=2016-06-26\n",
            "처리 중: platform=1, event_date=2016-06-25\n",
            "처리 중: platform=2, event_date=2016-06-25\n",
            "처리 중: platform=3, event_date=2016-06-25\n",
            "처리 중: platform=3, event_date=2016-06-26\n",
            "처리 중: platform=3, event_date=2016-06-27\n",
            "처리 중: platform=1, event_date=2016-06-27\n",
            "처리 중: platform=2, event_date=2016-06-27\n",
            "처리 중: platform=3, event_date=2016-06-28\n",
            "처리 중: platform=3, event_date=2016-06-29\n",
            "처리 중: platform=1, event_date=2016-06-28\n",
            "처리 중: platform=1, event_date=2016-06-29\n",
            "처리 중: platform=2, event_date=2016-06-28\n",
            "처리 중: platform=2, event_date=2016-06-29\n",
            "✅ 모든 파티션 정렬 완료!\n"
          ]
        }
      ],
      "source": [
        "output_path_final = \"../rawdata/events_partitioned\"\n",
        "\n",
        "# 1단계: platform, event_date 조합별로 개별 저장\n",
        "platforms_dates = df_events_processed.select(\"platform\", \"event_date\").distinct().collect()\n",
        "\n",
        "print(f\"총 {len(platforms_dates)}개의 파티션을 개별 정렬해서 저장합니다...\")\n",
        "\n",
        "for row in platforms_dates:\n",
        "    platform = row['platform']\n",
        "    event_date = row['event_date']\n",
        "    \n",
        "    print(f\"처리 중: platform={platform}, event_date={event_date}\")\n",
        "    \n",
        "    # 각 조합별로 필터링하고 정렬\n",
        "    df_partition = (df_events_processed\n",
        "        .filter((F.col(\"platform\") == platform) & (F.col(\"event_date\") == event_date))\n",
        "        .orderBy(\"timestamp\")  # timestamp만 정렬 (이미 platform, date로 필터됨)\n",
        "        .coalesce(1)  # 단일 파일로 강제\n",
        "    )\n",
        "    \n",
        "    # 개별 저장 (파티션 구조 유지)\n",
        "    partition_path = f\"{output_path_final}/platform={platform}/event_date={event_date}\"\n",
        "    df_partition.write.mode(\"overwrite\").parquet(partition_path)\n",
        "\n",
        "print(\"✅ 모든 파티션 정렬 완료!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 정렬확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 정렬 확인:\n",
            "첫 20개 레코드의 timestamp 순서:\n",
            "+---------+\n",
            "|timestamp|\n",
            "+---------+\n",
            "|849600071|\n",
            "|849600117|\n",
            "|849600157|\n",
            "|849600161|\n",
            "|849600206|\n",
            "|849600250|\n",
            "|849600303|\n",
            "|849600504|\n",
            "|849600723|\n",
            "|849600748|\n",
            "|849601085|\n",
            "|849601221|\n",
            "|849601498|\n",
            "|849601587|\n",
            "|849601697|\n",
            "|849601871|\n",
            "|849602045|\n",
            "|849602374|\n",
            "|849602453|\n",
            "|849602660|\n",
            "+---------+\n",
            "only showing top 20 rows\n",
            "\n",
            "✅ 정렬 여부: True\n"
          ]
        }
      ],
      "source": [
        "print(\"🔍 정렬 확인:\")\n",
        "target_path = \"../rawdata/events_partitioned/platform=2/event_date=2016-06-24/*.parquet\"\n",
        "\n",
        "df_check = spark.read.parquet(target_path)\n",
        "print(\"첫 20개 레코드의 timestamp 순서:\")\n",
        "df_check.select(\"timestamp\").show(20)\n",
        "\n",
        "# timestamp가 정렬되었는지 확인\n",
        "timestamps = df_check.select(\"timestamp\").collect()\n",
        "is_sorted = all(timestamps[i].timestamp <= timestamps[i+1].timestamp for i in range(len(timestamps)-1))\n",
        "print(f\"✅ 정렬 여부: {is_sorted}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `page_views.csv` 전처리 in AWS GLUE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import sys\n",
        "# from awsglue.transforms import *\n",
        "# from awsglue.utils import getResolvedOptions\n",
        "# from pyspark.context import SparkContext\n",
        "# from awsglue.context import GlueContext\n",
        "# from awsglue.job import Job\n",
        "\n",
        "# from pyspark.sql import functions as F\n",
        "\n",
        "# ## @params: [JOB_NAME]\n",
        "# args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
        "# SRC = \"s3://data-engineering-project-argus/page_views.csv\"\n",
        "# DST = \"s3://data-engineering-project-argus/page_views_partitioned\"\n",
        "\n",
        "# sc = SparkContext()\n",
        "# glueContext = GlueContext(sc)\n",
        "# spark = glueContext.spark_session\n",
        "\n",
        "# job = Job(glueContext)\n",
        "# job.init(args['JOB_NAME'], args)\n",
        "\n",
        "# df = (spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(SRC))\n",
        "\n",
        "# OFFSET = 1465876799998\n",
        "\n",
        "# # timestamp를 기반으로 event_time과 event_date 컬럼 생성\n",
        "# df_processed = (df\n",
        "#     .withColumn(\"event_time\", ((F.col(\"timestamp\") + OFFSET)/1000).cast(\"timestamp\"))\n",
        "#     .withColumn(\"event_date\", F.to_date(\"event_time\"))\n",
        "# )\n",
        "\n",
        "# platforms_dates = df_processed.select(\"platform\", \"event_date\").distinct().collect()\n",
        "\n",
        "# for row in platforms_dates:\n",
        "#     platform = row['platform']\n",
        "#     event_date = row['event_date']\n",
        "    \n",
        "    \n",
        "#     # 각 조합별로 필터링하고 정렬\n",
        "#     df_partition = (df_processed\n",
        "#         .filter((F.col(\"platform\") == platform) & (F.col(\"event_date\") == event_date))\n",
        "#         .orderBy(\"timestamp\")  # timestamp만 정렬 (이미 platform, date로 필터됨)\n",
        "#         .coalesce(1)  # 단일 파일로 강제\n",
        "#     )\n",
        "    \n",
        "#     # 개별 저장 (파티션 구조 유지)\n",
        "#     partition_path = f\"{output_path_final}/platform={platform}/event_date={event_date}\"\n",
        "#     df_partition.write.mode(\"overwrite\").parquet(DST)\n",
        "\n",
        "\n",
        "# job.commit()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
