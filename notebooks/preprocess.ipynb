{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `events.csv` ì „ì²˜ë¦¬"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° SparkSession ì„¤ì •\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "spark = SparkSession.builder.appName(\"DE_project\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Events ë°ì´í„° ìŠ¤í‚¤ë§ˆ:\n",
            "root\n",
            " |-- display_id: integer (nullable = true)\n",
            " |-- uuid: string (nullable = true)\n",
            " |-- document_id: integer (nullable = true)\n",
            " |-- timestamp: integer (nullable = true)\n",
            " |-- platform: string (nullable = true)\n",
            " |-- geo_location: string (nullable = true)\n",
            "\n",
            "\n",
            "ìƒ˜í”Œ ë°ì´í„°:\n",
            "+----------+--------------+-----------+---------+--------+------------+\n",
            "|display_id|          uuid|document_id|timestamp|platform|geo_location|\n",
            "+----------+--------------+-----------+---------+--------+------------+\n",
            "|         1|cb8c55702adb93|     379743|       61|       3|   US>SC>519|\n",
            "|         2|79a85fa78311b9|    1794259|       81|       2|   US>CA>807|\n",
            "|         3|822932ce3d8757|    1179111|      182|       2|   US>MI>505|\n",
            "|         4|85281d0a49f7ac|    1777797|      234|       2|   US>WV>564|\n",
            "|         5|8d0daef4bf5b56|     252458|      338|       2|       SG>00|\n",
            "+----------+--------------+-----------+---------+--------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# events ë°ì´í„° ì½ê¸° ë° êµ¬ì¡° í™•ì¸\n",
        "df_events = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"../rawdata/events.csv\")\n",
        "\n",
        "print(\"Events ë°ì´í„° ìŠ¤í‚¤ë§ˆ:\")\n",
        "df_events.printSchema()\n",
        "print(\"\\nìƒ˜í”Œ ë°ì´í„°:\")\n",
        "df_events.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### partition ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©í•  event_date ì»¬ëŸ¼ ìƒì„±"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì²˜ë¦¬ëœ ë°ì´í„° ìƒ˜í”Œ:\n",
            "+---------+--------------------+----------+--------+\n",
            "|timestamp|          event_time|event_date|platform|\n",
            "+---------+--------------------+----------+--------+\n",
            "|       61|2016-06-14 04:00:...|2016-06-14|       3|\n",
            "|       81|2016-06-14 04:00:...|2016-06-14|       2|\n",
            "|      182|2016-06-14 04:00:...|2016-06-14|       2|\n",
            "|      234|2016-06-14 04:00:...|2016-06-14|       2|\n",
            "|      338|2016-06-14 04:00:...|2016-06-14|       2|\n",
            "+---------+--------------------+----------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# timestampë¥¼ ê¸°ë°˜ìœ¼ë¡œ event_timeê³¼ event_date ì»¬ëŸ¼ ìƒì„±\n",
        "df_events_processed = (df_events\n",
        "    .withColumn(\"event_time\", ((F.col(\"timestamp\") + 1465876799998)/1000).cast(\"timestamp\"))\n",
        "    .withColumn(\"event_date\", F.to_date(\"event_time\"))\n",
        ")\n",
        "\n",
        "# ìƒ˜í”Œë¡œ ì²˜ë¦¬ëœ ë°ì´í„° í™•ì¸\n",
        "print(\"ì²˜ë¦¬ëœ ë°ì´í„° ìƒ˜í”Œ:\")\n",
        "df_events_processed.select(\"timestamp\", \"event_time\", \"event_date\", \"platform\").show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### platformì´ ëˆ„ë½ëœ ë ˆì½”ë“œ í™•ì¸ ë° ì…ë ¥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ” Platformì´ 1, 2, 3ì´ ì•„ë‹Œ ë°ì´í„°:\n",
            "+----------+--------------+-----------+---------+--------+------------+--------------------+----------+\n",
            "|display_id|          uuid|document_id|timestamp|platform|geo_location|          event_time|event_date|\n",
            "+----------+--------------+-----------+---------+--------+------------+--------------------+----------+\n",
            "|    303066|83e9ec48908c6a|     968149| 28799999|      \\N|   US>VA>511|2016-06-14 11:59:...|2016-06-14|\n",
            "|  11328496|7e8aa06b36db6a|    1827718|752399961|      \\N|   US>KY>529|2016-06-22 20:59:...|2016-06-22|\n",
            "|  13489553|5cda9845a1b9be|    2624774|896400000|      \\N|   US>IN>527|2016-06-24 12:59:...|2016-06-24|\n",
            "|  14004328|f4bb634c3871b9|     635051|921599944|      \\N|   US>NY>501|2016-06-24 19:59:...|2016-06-24|\n",
            "|  15056922|558ba104e8a37c|    2746860|997199999|      \\N|   US>MA>506|2016-06-25 16:59:...|2016-06-25|\n",
            "+----------+--------------+-----------+---------+--------+------------+--------------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"ğŸ” Platformì´ 1, 2, 3ì´ ì•„ë‹Œ ë°ì´í„°:\")\n",
        "df_events_processed.filter(~F.col(\"platform\").isin(['1', '2', '3'])).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# platform missing value ì²˜ë¦¬ based on page_views.csv\n",
        "# +----------+--------------+-----------+---------+--------+------------+\n",
        "# |display_id|          uuid|document_id|timestamp|platform|geo_location|\n",
        "# +----------+--------------+-----------+---------+--------+------------+\n",
        "# |    303066|83e9ec48908c6a|     968149| 28799999|      \\N|   US>VA>511| -> platform 2\n",
        "# |  11328496|7e8aa06b36db6a|    1827718|752399961|      \\N|   US>KY>529| -> platform 1\n",
        "# |  13489553|5cda9845a1b9be|    2624774|896400000|      \\N|   US>IN>527| -> platform 3\n",
        "# |  14004328|f4bb634c3871b9|     635051|921599944|      \\N|   US>NY>501| -> platform 1\n",
        "# |  15056922|558ba104e8a37c|    2746860|997199999|      \\N|   US>MA>506| -> platform 1\n",
        "# +----------+--------------+-----------+---------+--------+------------+\n",
        "\n",
        "df_events_processed = df_events_processed.withColumn(\"platform\",\n",
        "    F.when(F.col(\"display_id\").isin([11328496, 14004328, 15056922]), \"1\")\n",
        "    .when(F.col(\"display_id\").isin([303066]), \"2\") \n",
        "    .when(F.col(\"display_id\").isin([13489553]), \"3\")\n",
        "    .otherwise(F.col(\"platform\"))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+--------------+-----------+---------+--------+------------+--------------------+----------+\n",
            "|display_id|          uuid|document_id|timestamp|platform|geo_location|          event_time|event_date|\n",
            "+----------+--------------+-----------+---------+--------+------------+--------------------+----------+\n",
            "|    303066|83e9ec48908c6a|     968149| 28799999|       2|   US>VA>511|2016-06-14 11:59:...|2016-06-14|\n",
            "|  11328496|7e8aa06b36db6a|    1827718|752399961|       1|   US>KY>529|2016-06-22 20:59:...|2016-06-22|\n",
            "|  13489553|5cda9845a1b9be|    2624774|896400000|       3|   US>IN>527|2016-06-24 12:59:...|2016-06-24|\n",
            "|  14004328|f4bb634c3871b9|     635051|921599944|       1|   US>NY>501|2016-06-24 19:59:...|2016-06-24|\n",
            "|  15056922|558ba104e8a37c|    2746860|997199999|       1|   US>MA>506|2016-06-25 16:59:...|2016-06-25|\n",
            "+----------+--------------+-----------+---------+--------+------------+--------------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# í™•ì¸\n",
        "df_events_processed.filter(F.col(\"display_id\").isin([303066, 11328496, 13489553, 14004328, 15056922])).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### platform - event_date êµ¬ì¡°ë¡œ ë‚˜ëˆ„ê³  timestampë¡œ ì •ë ¬í•´ì„œ parquetìœ¼ë¡œ ì €ì¥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì´ 48ê°œì˜ íŒŒí‹°ì…˜ì„ ê°œë³„ ì •ë ¬í•´ì„œ ì €ì¥í•©ë‹ˆë‹¤...\n",
            "ì²˜ë¦¬ ì¤‘: platform=3, event_date=2016-06-15\n",
            "ì²˜ë¦¬ ì¤‘: platform=1, event_date=2016-06-14\n",
            "ì²˜ë¦¬ ì¤‘: platform=2, event_date=2016-06-15\n",
            "ì²˜ë¦¬ ì¤‘: platform=2, event_date=2016-06-14\n",
            "ì²˜ë¦¬ ì¤‘: platform=3, event_date=2016-06-14\n",
            "ì²˜ë¦¬ ì¤‘: platform=1, event_date=2016-06-15\n",
            "ì²˜ë¦¬ ì¤‘: platform=2, event_date=2016-06-16\n",
            "ì²˜ë¦¬ ì¤‘: platform=3, event_date=2016-06-17\n",
            "ì²˜ë¦¬ ì¤‘: platform=1, event_date=2016-06-17\n",
            "ì²˜ë¦¬ ì¤‘: platform=2, event_date=2016-06-17\n",
            "ì²˜ë¦¬ ì¤‘: platform=1, event_date=2016-06-16\n",
            "ì²˜ë¦¬ ì¤‘: platform=3, event_date=2016-06-16\n",
            "ì²˜ë¦¬ ì¤‘: platform=3, event_date=2016-06-18\n",
            "ì²˜ë¦¬ ì¤‘: platform=2, event_date=2016-06-19\n",
            "ì²˜ë¦¬ ì¤‘: platform=3, event_date=2016-06-19\n",
            "ì²˜ë¦¬ ì¤‘: platform=1, event_date=2016-06-19\n",
            "ì²˜ë¦¬ ì¤‘: platform=2, event_date=2016-06-18\n",
            "ì²˜ë¦¬ ì¤‘: platform=1, event_date=2016-06-18\n",
            "ì²˜ë¦¬ ì¤‘: platform=3, event_date=2016-06-20\n",
            "ì²˜ë¦¬ ì¤‘: platform=3, event_date=2016-06-21\n",
            "ì²˜ë¦¬ ì¤‘: platform=2, event_date=2016-06-21\n",
            "ì²˜ë¦¬ ì¤‘: platform=2, event_date=2016-06-20\n",
            "ì²˜ë¦¬ ì¤‘: platform=1, event_date=2016-06-20\n",
            "ì²˜ë¦¬ ì¤‘: platform=1, event_date=2016-06-21\n",
            "ì²˜ë¦¬ ì¤‘: platform=1, event_date=2016-06-23\n",
            "ì²˜ë¦¬ ì¤‘: platform=3, event_date=2016-06-22\n",
            "ì²˜ë¦¬ ì¤‘: platform=1, event_date=2016-06-22\n",
            "ì²˜ë¦¬ ì¤‘: platform=3, event_date=2016-06-23\n",
            "ì²˜ë¦¬ ì¤‘: platform=2, event_date=2016-06-22\n",
            "ì²˜ë¦¬ ì¤‘: platform=2, event_date=2016-06-23\n",
            "ì²˜ë¦¬ ì¤‘: platform=1, event_date=2016-06-24\n",
            "ì²˜ë¦¬ ì¤‘: platform=2, event_date=2016-06-24\n",
            "ì²˜ë¦¬ ì¤‘: platform=3, event_date=2016-06-24\n",
            "ì²˜ë¦¬ ì¤‘: platform=2, event_date=2016-06-26\n",
            "ì²˜ë¦¬ ì¤‘: platform=1, event_date=2016-06-26\n",
            "ì²˜ë¦¬ ì¤‘: platform=1, event_date=2016-06-25\n",
            "ì²˜ë¦¬ ì¤‘: platform=2, event_date=2016-06-25\n",
            "ì²˜ë¦¬ ì¤‘: platform=3, event_date=2016-06-25\n",
            "ì²˜ë¦¬ ì¤‘: platform=3, event_date=2016-06-26\n",
            "ì²˜ë¦¬ ì¤‘: platform=3, event_date=2016-06-27\n",
            "ì²˜ë¦¬ ì¤‘: platform=1, event_date=2016-06-27\n",
            "ì²˜ë¦¬ ì¤‘: platform=2, event_date=2016-06-27\n",
            "ì²˜ë¦¬ ì¤‘: platform=3, event_date=2016-06-28\n",
            "ì²˜ë¦¬ ì¤‘: platform=3, event_date=2016-06-29\n",
            "ì²˜ë¦¬ ì¤‘: platform=1, event_date=2016-06-28\n",
            "ì²˜ë¦¬ ì¤‘: platform=1, event_date=2016-06-29\n",
            "ì²˜ë¦¬ ì¤‘: platform=2, event_date=2016-06-28\n",
            "ì²˜ë¦¬ ì¤‘: platform=2, event_date=2016-06-29\n",
            "âœ… ëª¨ë“  íŒŒí‹°ì…˜ ì •ë ¬ ì™„ë£Œ!\n"
          ]
        }
      ],
      "source": [
        "output_path_final = \"../rawdata/events_partitioned\"\n",
        "\n",
        "# 1ë‹¨ê³„: platform, event_date ì¡°í•©ë³„ë¡œ ê°œë³„ ì €ì¥\n",
        "platforms_dates = df_events_processed.select(\"platform\", \"event_date\").distinct().collect()\n",
        "\n",
        "print(f\"ì´ {len(platforms_dates)}ê°œì˜ íŒŒí‹°ì…˜ì„ ê°œë³„ ì •ë ¬í•´ì„œ ì €ì¥í•©ë‹ˆë‹¤...\")\n",
        "\n",
        "for row in platforms_dates:\n",
        "    platform = row['platform']\n",
        "    event_date = row['event_date']\n",
        "    \n",
        "    print(f\"ì²˜ë¦¬ ì¤‘: platform={platform}, event_date={event_date}\")\n",
        "    \n",
        "    # ê° ì¡°í•©ë³„ë¡œ í•„í„°ë§í•˜ê³  ì •ë ¬\n",
        "    df_partition = (df_events_processed\n",
        "        .filter((F.col(\"platform\") == platform) & (F.col(\"event_date\") == event_date))\n",
        "        .orderBy(\"timestamp\")  # timestampë§Œ ì •ë ¬ (ì´ë¯¸ platform, dateë¡œ í•„í„°ë¨)\n",
        "        .coalesce(1)  # ë‹¨ì¼ íŒŒì¼ë¡œ ê°•ì œ\n",
        "    )\n",
        "    \n",
        "    # ê°œë³„ ì €ì¥ (íŒŒí‹°ì…˜ êµ¬ì¡° ìœ ì§€)\n",
        "    partition_path = f\"{output_path_final}/platform={platform}/event_date={event_date}\"\n",
        "    df_partition.write.mode(\"overwrite\").parquet(partition_path)\n",
        "\n",
        "print(\"âœ… ëª¨ë“  íŒŒí‹°ì…˜ ì •ë ¬ ì™„ë£Œ!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### ì •ë ¬í™•ì¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ” ì •ë ¬ í™•ì¸:\n",
            "ì²« 20ê°œ ë ˆì½”ë“œì˜ timestamp ìˆœì„œ:\n",
            "+---------+\n",
            "|timestamp|\n",
            "+---------+\n",
            "|849600071|\n",
            "|849600117|\n",
            "|849600157|\n",
            "|849600161|\n",
            "|849600206|\n",
            "|849600250|\n",
            "|849600303|\n",
            "|849600504|\n",
            "|849600723|\n",
            "|849600748|\n",
            "|849601085|\n",
            "|849601221|\n",
            "|849601498|\n",
            "|849601587|\n",
            "|849601697|\n",
            "|849601871|\n",
            "|849602045|\n",
            "|849602374|\n",
            "|849602453|\n",
            "|849602660|\n",
            "+---------+\n",
            "only showing top 20 rows\n",
            "\n",
            "âœ… ì •ë ¬ ì—¬ë¶€: True\n"
          ]
        }
      ],
      "source": [
        "print(\"ğŸ” ì •ë ¬ í™•ì¸:\")\n",
        "target_path = \"../rawdata/events_partitioned/platform=2/event_date=2016-06-24/*.parquet\"\n",
        "\n",
        "df_check = spark.read.parquet(target_path)\n",
        "print(\"ì²« 20ê°œ ë ˆì½”ë“œì˜ timestamp ìˆœì„œ:\")\n",
        "df_check.select(\"timestamp\").show(20)\n",
        "\n",
        "# timestampê°€ ì •ë ¬ë˜ì—ˆëŠ”ì§€ í™•ì¸\n",
        "timestamps = df_check.select(\"timestamp\").collect()\n",
        "is_sorted = all(timestamps[i].timestamp <= timestamps[i+1].timestamp for i in range(len(timestamps)-1))\n",
        "print(f\"âœ… ì •ë ¬ ì—¬ë¶€: {is_sorted}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `page_views.csv` ì „ì²˜ë¦¬ in AWS GLUE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import sys\n",
        "# from awsglue.transforms import *\n",
        "# from awsglue.utils import getResolvedOptions\n",
        "# from pyspark.context import SparkContext\n",
        "# from awsglue.context import GlueContext\n",
        "# from awsglue.job import Job\n",
        "\n",
        "# from pyspark.sql import functions as F\n",
        "\n",
        "# ## @params: [JOB_NAME]\n",
        "# args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
        "# SRC = \"s3://data-engineering-project-argus/page_views.csv\"\n",
        "# DST = \"s3://data-engineering-project-argus/page_views_partitioned\"\n",
        "\n",
        "# sc = SparkContext()\n",
        "# glueContext = GlueContext(sc)\n",
        "# spark = glueContext.spark_session\n",
        "\n",
        "# job = Job(glueContext)\n",
        "# job.init(args['JOB_NAME'], args)\n",
        "\n",
        "# df = (spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(SRC))\n",
        "\n",
        "# OFFSET = 1465876799998\n",
        "\n",
        "# # timestampë¥¼ ê¸°ë°˜ìœ¼ë¡œ event_timeê³¼ event_date ì»¬ëŸ¼ ìƒì„±\n",
        "# df_processed = (df\n",
        "#     .withColumn(\"event_time\", ((F.col(\"timestamp\") + OFFSET)/1000).cast(\"timestamp\"))\n",
        "#     .withColumn(\"event_date\", F.to_date(\"event_time\"))\n",
        "# )\n",
        "\n",
        "# platforms_dates = df_processed.select(\"platform\", \"event_date\").distinct().collect()\n",
        "\n",
        "# for row in platforms_dates:\n",
        "#     platform = row['platform']\n",
        "#     event_date = row['event_date']\n",
        "    \n",
        "    \n",
        "#     # ê° ì¡°í•©ë³„ë¡œ í•„í„°ë§í•˜ê³  ì •ë ¬\n",
        "#     df_partition = (df_processed\n",
        "#         .filter((F.col(\"platform\") == platform) & (F.col(\"event_date\") == event_date))\n",
        "#         .orderBy(\"timestamp\")  # timestampë§Œ ì •ë ¬ (ì´ë¯¸ platform, dateë¡œ í•„í„°ë¨)\n",
        "#         .coalesce(1)  # ë‹¨ì¼ íŒŒì¼ë¡œ ê°•ì œ\n",
        "#     )\n",
        "    \n",
        "#     # ê°œë³„ ì €ì¥ (íŒŒí‹°ì…˜ êµ¬ì¡° ìœ ì§€)\n",
        "#     partition_path = f\"{output_path_final}/platform={platform}/event_date={event_date}\"\n",
        "#     df_partition.write.mode(\"overwrite\").parquet(DST)\n",
        "\n",
        "\n",
        "# job.commit()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
