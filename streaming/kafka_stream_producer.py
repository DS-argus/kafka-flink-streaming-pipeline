#!/usr/bin/env python3

import json
import logging
import os
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional

import pytz

KST = pytz.timezone("Asia/Seoul")

import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import yaml
from confluent_kafka import Producer
from confluent_kafka.admin import AdminClient


# ì„¤ì • íŒŒì¼ ë¡œë“œ
def load_config(config_path: str) -> dict:
    """YAML ì„¤ì • íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        with open(config_path, "r", encoding="utf-8") as file:
            config = yaml.safe_load(file)
        return config
    except FileNotFoundError:
        print(f"âŒ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_path}")
        return {}
    except yaml.YAMLError as e:
        print(f"âŒ YAML íŒŒì¼ íŒŒì‹± ì˜¤ë¥˜: {e}")
        return {}


# ì„¤ì • ë¡œë“œ
CONFIG = load_config("kafka_config.yml")


# í˜„ì¬ ì»¨í…Œì´ë„ˆì˜ í”Œë«í¼ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
PLATFORM_NAME = os.getenv("PLATFORM_NAME")  # desktop, mobile, tablet
if not PLATFORM_NAME:
    raise ValueError("PLATFORM_NAME í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

platform_config = (
    CONFIG.get("environment", {}).get("platforms", {}).get(PLATFORM_NAME, {})
)
if not platform_config:
    raise ValueError(f"í”Œë«í¼ '{PLATFORM_NAME}'ì— ëŒ€í•œ ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# í”Œë«í¼ë³„ ì„¤ì •
PLATFORM_ID = int(platform_config.get("platform_id"))
if not PLATFORM_ID:
    raise ValueError(f"í”Œë«í¼ '{PLATFORM_NAME}'ì˜ platform_idê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

SELECTED_PLATFORMS = platform_config.get("selected_platforms")
if not SELECTED_PLATFORMS:
    raise ValueError(
        f"í”Œë«í¼ '{PLATFORM_NAME}'ì˜ selected_platformsê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    )

# ê³µí†µ ì„¤ì •
ENV_CONFIG = CONFIG.get("environment", {})
START_DATE = ENV_CONFIG.get("start_date")
if not START_DATE:
    raise ValueError("start_dateê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

END_DATE = ENV_CONFIG.get("end_date")
if not END_DATE:
    raise ValueError("end_dateê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

TIME_SCALE_FACTOR = float(ENV_CONFIG.get("time_scale_factor"))
if TIME_SCALE_FACTOR is None:
    raise ValueError("time_scale_factorê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# Kafka ì„¤ì •
KAFKA_CONFIG = CONFIG.get("kafka", {})
KAFKA_BOOTSTRAP_SERVERS = KAFKA_CONFIG.get("bootstrap_servers")
if not KAFKA_BOOTSTRAP_SERVERS:
    raise ValueError("kafka.bootstrap_serversê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

STREAMING_CONFIG = CONFIG.get("streaming", {})


# logs í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
os.makedirs("logs", exist_ok=True)

# ë¡œê·¸ íŒŒì¼ ì´ë¦„ (ë‚ ì§œ/ì‹œê°„ í¬í•¨)
log_filename = f"logs/kafka_stream_{PLATFORM_NAME.lower()}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# í¬ë§·í„° ì„¤ì •
formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")

# íŒŒì¼ í•¸ë“¤ëŸ¬ (íŒŒì¼ì— ë¡œê·¸ ì €ì¥)
file_handler = logging.FileHandler(log_filename, encoding="utf-8")
file_handler.setFormatter(formatter)
file_handler.setLevel(logging.INFO)

# ì½˜ì†” í•¸ë“¤ëŸ¬ (í„°ë¯¸ë„ì— ë¡œê·¸ ì¶œë ¥)
console_handler = logging.StreamHandler()
console_handler.setFormatter(formatter)
console_handler.setLevel(logging.INFO)

# ë¡œê±°ì— í•¸ë“¤ëŸ¬ ì¶”ê°€
logger.addHandler(file_handler)
logger.addHandler(console_handler)


class KafkaDataStreamer:
    def __init__(self, bootstrap_servers=None):
        """
        Kafka ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë¨¸ ì´ˆê¸°í™”

        Args:
            bootstrap_servers: Kafka ë¸Œë¡œì»¤ ì£¼ì†Œ ë¦¬ìŠ¤íŠ¸
        """
        # bootstrap_servers ì„¤ì •
        if bootstrap_servers is None:
            bootstrap_servers = KAFKA_BOOTSTRAP_SERVERS.split(",")

        self.bootstrap_servers = bootstrap_servers
        self.admin_client: Optional[AdminClient] = None
        self.events_producer = None  # events ì „ìš© producer
        self.page_views_producer = None  # page_views ì „ìš© producer
        self.topics = ["events", "page_views"]

        # í˜„ì¬ í”Œë«í¼ ì„¤ì •
        self.platform_id = PLATFORM_ID
        self.platform_name = PLATFORM_NAME

        # ë°ì´í„° ê²½ë¡œ ì„¤ì •
        self.events_path = Path("rawdata/events_partitioned")
        self.page_views_path = Path("rawdata/page_views_partitioned")

        # ë‚ ì§œ ë²”ìœ„ ì„¤ì •
        self.start_date = datetime.strptime(START_DATE, "%Y-%m-%d")
        self.end_date = datetime.strptime(END_DATE, "%Y-%m-%d")

        # ì‹œê°„ ìŠ¤ì¼€ì¼ íŒ©í„° ì„¤ì •
        self.time_scale_factor = TIME_SCALE_FACTOR

        # ì»¬ëŸ¼ ì •ì˜
        self.events_columns = [
            "display_id",
            "uuid",
            "document_id",
            "timestamp",
            "geo_location",
        ]

        self.page_views_columns = [
            "uuid",
            "document_id",
            "timestamp",
            "geo_location",
            "traffic_source",
        ]

        logger.info(f"ğŸš€ {PLATFORM_NAME} ìŠ¤íŠ¸ë¦¬ë¨¸ ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"   - í”Œë«í¼ ID: {self.platform_id}")
        logger.info(
            f"   - ë‚ ì§œ ë²”ìœ„: {self.start_date.strftime('%Y-%m-%d')} ~ {self.end_date.strftime('%Y-%m-%d')}"
        )
        logger.info(f"   - ì‹œê°„ ìŠ¤ì¼€ì¼ íŒ©í„°: {self.time_scale_factor}")

    def connect_kafka(self):
        """Kafkaì— ì—°ê²°í•˜ê³  í•„ìš”í•œ ì»´í¬ë„ŒíŠ¸ë“¤ì„ ì´ˆê¸°í™”"""
        try:
            # Admin í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            admin_config = {
                "bootstrap.servers": ",".join(self.bootstrap_servers),
                "client.id": "data_streamer_admin",
            }
            self.admin_client = AdminClient(admin_config)
            logger.info("âœ… Kafka Admin í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì„±ê³µ")

            # Events ì „ìš© Producer ìƒì„±
            producer_config = KAFKA_CONFIG.get("producer", {})
            events_config = producer_config.copy()
            events_config.update(KAFKA_CONFIG.get("events_producer"))
            events_config.update(
                {
                    "bootstrap.servers": ",".join(self.bootstrap_servers),
                }
            )
            self.events_producer = Producer(events_config)

            # Page Views ì „ìš© Producer ìƒì„±
            page_views_config = producer_config.copy()
            page_views_config.update(KAFKA_CONFIG.get("page_views_producer"))
            page_views_config.update(
                {
                    "bootstrap.servers": ",".join(self.bootstrap_servers),
                }
            )
            self.page_views_producer = Producer(page_views_config)

            logger.info(
                f"âœ… {PLATFORM_NAME} Producer ìƒì„± ì™„ë£Œ (Eventsìš©, Page Viewsìš©)"
            )
            return True

        except Exception as e:
            logger.error(f"âŒ Kafka ì—°ê²° ì‹¤íŒ¨: {e}")
            return False

    def calculate_sleep_duration(
        self, current_timestamp, next_timestamp, time_scale_factor
    ):
        """ì‹¤ì œ timestamp ì°¨ì´ì— ë§ì¶°ì„œ sleep ì‹œê°„ ê³„ì‚°"""
        if pd.isna(current_timestamp) or pd.isna(next_timestamp):
            return 0.001

        # timestamp ì°¨ì´ (s)
        time_diff = (next_timestamp - current_timestamp) / 1000

        # ì‹œê°„ ìŠ¤ì¼€ì¼ íŒ©í„° ì ìš© (ì‹¤ì œ ì‹œê°„ì„ ë” ë§ì´ ì¶•ì†Œ)
        sleep_duration = time_diff * time_scale_factor

        # ìµœì†Œ/ìµœëŒ€ sleep ì‹œê°„ ì œí•œ
        sleep_duration = max(0.001, sleep_duration)

        return sleep_duration

    def get_date_range(self) -> List[datetime]:
        """ì²˜ë¦¬í•  ë‚ ì§œ ë²”ìœ„ ë°˜í™˜"""
        dates = []
        current_date = self.start_date

        while current_date <= self.end_date:
            dates.append(current_date)
            current_date += timedelta(days=1)

        return dates

    def get_parquet_files(self, date: datetime, data_type: str) -> Optional[Path]:
        """íŠ¹ì • ë‚ ì§œì˜ í˜„ì¬ í”Œë«í¼ parquet íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
        date_str = date.strftime("%Y-%m-%d")
        base_path = self.events_path if data_type == "events" else self.page_views_path
        platform_path = (
            base_path / f"platform={self.platform_id}" / f"event_date={date_str}"
        )

        if platform_path.exists():
            parquet_files = list(platform_path.glob("*.parquet"))
            if parquet_files:
                return parquet_files[0]
            else:
                logger.warning(f"âš ï¸  Parquet íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {platform_path}")
        else:
            logger.warning(f"âš ï¸  ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {platform_path}")

        return None

    def on_send_success(self, err, msg):
        """ì „ì†¡ ì„±ê³µ ì‹œ í˜¸ì¶œë˜ëŠ” ì½œë°± í•¨ìˆ˜"""
        if err is not None:
            logger.error(f"âŒ ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨: {err}")
        else:
            # logger.info(
            #     f"âœ… ë©”ì‹œì§€ ì „ì†¡ ì„±ê³µ: íŒŒí‹°ì…˜ {msg.partition()}, ì˜¤í”„ì…‹ {msg.offset()}"
            # )
            pass

    def on_send_error(self, err, msg):
        """ì „ì†¡ ì‹¤íŒ¨ ì‹œ í˜¸ì¶œë˜ëŠ” ì½œë°± í•¨ìˆ˜"""
        logger.error(f"âŒ ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨: {err}")

    def produce_data_batch(
        self, events_file: Optional[Path], page_views_file: Optional[Path]
    ):
        """ë°°ì¹˜ ë°©ì‹ìœ¼ë¡œ page_viewsë¥¼ ë©”ì¸ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°í•˜ê³ , eventsëŠ” timestamp ë¹„êµë¡œ ì¤‘ê°„ì— ì‚½ì…"""
        if (events_file is None or not events_file.exists()) and (
            page_views_file is None or not page_views_file.exists()
        ):
            return

        # Producerê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš° ì²˜ë¦¬
        if self.events_producer is None or self.page_views_producer is None:
            logger.error("âŒ Producerê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return

        # Events ì „ì²´ ë°ì´í„° ë¡œë“œ (í¬ê¸°ê°€ ì‘ê¸° ë•Œë¬¸ì—)
        events_df = None
        if events_file and events_file.exists():
            events_parquet = pq.ParquetFile(events_file)
            events_df = events_parquet.read(columns=self.events_columns).to_pandas()
            logger.info(f"ğŸ“¦ Events ì „ì²´ ë¡œë“œ: {len(events_df)}ê°œ ë ˆì½”ë“œ")

        # Page Views ë°°ì¹˜ ë°˜ë³µì ì´ˆê¸°í™”
        page_views_parquet = (
            pq.ParquetFile(page_views_file)
            if page_views_file and page_views_file.exists()
            else None
        )
        page_views_batches = (
            page_views_parquet.iter_batches(
                batch_size=50000, columns=self.page_views_columns
            )
            if page_views_parquet
            else None
        )

        # í˜„ì¬ ë°°ì¹˜ ë°ì´í„°
        page_views_df = None
        page_views_idx = 0
        events_idx = 0

        batch_count = 0
        events_count = 0
        page_views_count = 0
        start_time = time.time()
        prev_time = None

        logger.info(f"ğŸ“¤ {PLATFORM_NAME}: ë°°ì¹˜ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘")

        try:
            while True:
                # Page Views ë°°ì¹˜ ë¡œë“œ (í•„ìš”í•œ ê²½ìš°)
                if page_views_df is None or page_views_idx >= len(page_views_df):
                    if page_views_batches:
                        try:
                            page_views_batch = next(page_views_batches)
                            page_views_df = page_views_batch.to_pandas()
                            page_views_idx = 0
                            logger.info(
                                f"ğŸ“¦ Page Views ë°°ì¹˜ ë¡œë“œ: {len(page_views_df)}ê°œ ë ˆì½”ë“œ"
                            )
                        except StopIteration:
                            page_views_df = None
                            page_views_batches = None
                    else:
                        page_views_df = None

                # ë” ì´ìƒ ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
                if page_views_df is None and (
                    events_df is None or events_idx >= len(events_df)
                ):
                    break

                # Page Viewsë¥¼ ë©”ì¸ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°
                while page_views_df is not None and page_views_idx < len(page_views_df):
                    current_page_views_time = page_views_df.iloc[page_views_idx][
                        "timestamp"
                    ]

                    # Eventsì˜ ë‹¤ìŒ timestamp í™•ì¸
                    next_events_time = float("inf")
                    if events_df is not None and events_idx < len(events_df):
                        next_events_time = events_df.iloc[events_idx]["timestamp"]

                    # Page Views timestampê°€ Events ë‹¤ìŒ timestampë³´ë‹¤ í¬ë©´ Events ìŠ¤íŠ¸ë¦¬ë°
                    if (
                        current_page_views_time > next_events_time
                        and events_df is not None
                        and events_idx < len(events_df)
                    ):
                        # Events ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘
                        while events_df is not None and events_idx < len(events_df):
                            events_time = events_df.iloc[events_idx]["timestamp"]

                            # Events timestampê°€ í˜„ì¬ Page Views timestampë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ì„ ë•Œë§Œ ìŠ¤íŠ¸ë¦¬ë°
                            if events_time <= current_page_views_time:
                                row = events_df.iloc[events_idx]
                                events_idx += 1
                                events_count += 1

                                # Events ë©”ì‹œì§€ ì „ì†¡
                                message = row.to_dict()
                                message["platform_id"] = self.platform_id
                                message["act_prod_time"] = datetime.now(KST).strftime(
                                    "%Y-%m-%d %H:%M:%S.%f"
                                )

                                message_json = json.dumps(message, default=str)
                                key = str(message.get("uuid", ""))

                                self.events_producer.produce(
                                    topic="events",
                                    key=key.encode("utf-8"),
                                    value=message_json.encode("utf-8"),
                                    callback=self.on_send_success,
                                )

                                batch_count += 1

                                # ì‹¤ì œ ì‹œê°„ ê°„ê²© ì‹œë®¬ë ˆì´ì…˜
                                if prev_time is not None:
                                    sleep_duration = self.calculate_sleep_duration(
                                        prev_time, events_time, self.time_scale_factor
                                    )
                                    time.sleep(sleep_duration)

                                prev_time = events_time
                            else:
                                break

                    # Page Views ë°ì´í„° ì „ì†¡
                    row = page_views_df.iloc[page_views_idx]
                    page_views_idx += 1
                    page_views_count += 1

                    # Page Views ë©”ì‹œì§€ ì „ì†¡
                    message = row.to_dict()
                    message["platform_id"] = self.platform_id
                    message["act_prod_time"] = datetime.now(KST).strftime(
                        "%Y-%m-%d %H:%M:%S.%f"
                    )

                    message_json = json.dumps(message, default=str)
                    key = str(message.get("uuid", ""))

                    self.page_views_producer.produce(
                        topic="page_views",
                        key=key.encode("utf-8"),
                        value=message_json.encode("utf-8"),
                        callback=self.on_send_success,
                    )

                    batch_count += 1

                    # ì‹¤ì œ ì‹œê°„ ê°„ê²© ì‹œë®¬ë ˆì´ì…˜
                    if prev_time is not None:
                        sleep_duration = self.calculate_sleep_duration(
                            prev_time, current_page_views_time, self.time_scale_factor
                        )
                        time.sleep(sleep_duration)

                    prev_time = current_page_views_time

                    # ì£¼ê¸°ì ì¸ poll : local queue full ì˜¤ë¥˜ ë°©ì§€? -> í•´ë„ ì•ˆë˜ë„¤?
                    if batch_count % 1000 == 0:
                        self.events_producer.poll(0)
                        self.page_views_producer.poll(0)

                # ì§„í–‰ìƒí™© ë¡œê·¸
                if batch_count % 100000 == 0:
                    elapsed_time = time.time() - start_time
                    logger.info(
                        f"  {PLATFORM_NAME} - ì „ì†¡ ì§„í–‰: {batch_count:,} "
                        f"(Events: {events_count:,}, Page Views: {page_views_count:,}) "
                        f"ê²½ê³¼ì‹œê°„: {elapsed_time:.2f}ì´ˆ"
                    )

        except Exception as e:
            logger.error(f"âŒ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        finally:
            # ìµœì¢… flush
            self.events_producer.flush()
            self.page_views_producer.flush()
            total_time = time.time() - start_time

            logger.info(
                f"âœ… {PLATFORM_NAME} - ë°°ì¹˜ ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ: ì´ {batch_count:,}ê°œ ë©”ì‹œì§€ "
                f"(Events: {events_count:,}, Page Views: {page_views_count:,}) "
                f"ì†Œìš”ì‹œê°„: {total_time:.2f}ì´ˆ, í‰ê·  TPS: {batch_count/total_time:.1f}"
            )

    def process_daily_data(self, date: datetime):
        """íŠ¹ì • ë‚ ì§œì˜ í˜„ì¬ í”Œë«í¼ ë°ì´í„°ë¥¼ ë°°ì¹˜ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬"""
        date_str = date.strftime("%Y-%m-%d")
        logger.info(f"\n{'='*70}")
        logger.info(f"ğŸ“… {date_str} {PLATFORM_NAME} ë°ì´í„° ì²˜ë¦¬ ì‹œì‘")
        logger.info(f"{'='*70}")

        # Eventsì™€ Page Views íŒŒì¼ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
        events_file = self.get_parquet_files(date, "events")
        page_views_file = self.get_parquet_files(date, "page_views")

        # ë°ì´í„° íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
        if events_file is None and page_views_file is None:
            logger.warning(
                f"âš ï¸  {PLATFORM_NAME}: Eventsì™€ Page Views ë°ì´í„°ê°€ ëª¨ë‘ ì—†ìŠµë‹ˆë‹¤."
            )
            return

        # ë°°ì¹˜ ë°©ì‹ìœ¼ë¡œ ë°ì´í„° ì²˜ë¦¬
        logger.info(f"ğŸš€ {PLATFORM_NAME}: ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘")
        self.produce_data_batch(events_file, page_views_file)

        logger.info(f"âœ… {date_str} {PLATFORM_NAME} ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ!")

    def start_streaming(self):
        """ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘"""
        logger.info(f"ğŸš€ {PLATFORM_NAME} Kafka ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘")

        # Kafka ì—°ê²°
        if not self.connect_kafka():
            logger.error("âŒ Kafka ì—°ê²° ì‹¤íŒ¨. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return

        # ë‚ ì§œë³„ ì²˜ë¦¬
        dates = self.get_date_range()
        logger.info(
            f"ğŸ“Š ì²˜ë¦¬í•  ë‚ ì§œ ë²”ìœ„: {len(dates)}ì¼ "
            f"({dates[0].strftime('%Y-%m-%d')} ~ {dates[-1].strftime('%Y-%m-%d')})"
        )

        for i, date in enumerate(dates):
            try:
                # í•´ë‹¹ ë‚ ì§œ ë°ì´í„° ì²˜ë¦¬
                self.process_daily_data(date)
                logger.info(f"âœ… {date.strftime('%Y-%m-%d')} ë°ì´í„° ì „ì†¡ ì™„ë£Œ!")

            except KeyboardInterrupt:
                logger.info("\nì‚¬ìš©ì ì¸í„°ëŸ½íŠ¸ë¡œ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                logger.error(f"âŒ ë‚ ì§œ {date.strftime('%Y-%m-%d')} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue

        logger.info("ğŸ‰ ëª¨ë“  ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ!")

        # Producer ì¢…ë£Œ
        if self.events_producer:
            self.events_producer.flush()
        if self.page_views_producer:
            self.page_views_producer.flush()


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print(f"ğŸ¯ {PLATFORM_NAME} Kafka ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë° Producer")
    print("=" * 70)

    # Kafka ì„œë²„ ì„¤ì •
    kafka_servers = KAFKA_BOOTSTRAP_SERVERS.split(",")

    print(f"ğŸ“¡ Kafka ì„œë²„: {kafka_servers}")
    print(f"ğŸ“‚ Events ë°ì´í„° ê²½ë¡œ: rawdata/events_partitioned/")
    print(f"ğŸ“‚ Page Views ë°ì´í„° ê²½ë¡œ: rawdata/page_views_partitioned/")
    print(f"ğŸ·ï¸  ìƒì„±í•  í† í”½: events, page_views")
    print(f"ğŸ“„ ë¡œê·¸ íŒŒì¼: {log_filename}")
    print("=" * 70)

    # ì‹œì‘ í™•ì¸
    print(f"\nğŸ“‹ ì„¤ì • ìš”ì•½:")
    print(f"   - í”Œë«í¼: {PLATFORM_NAME} (ID: {PLATFORM_ID})")
    print(f"   - ë‚ ì§œ ë²”ìœ„: {START_DATE} ~ {END_DATE}")
    print(f"   - ì‹œê°„ ìŠ¤ì¼€ì¼ íŒ©í„°: {TIME_SCALE_FACTOR}")

    # ìë™ ì‹œì‘
    print("\nğŸš€ ìŠ¤íŠ¸ë¦¬ë°ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    # ìŠ¤íŠ¸ë¦¬ë¨¸ ìƒì„± ë° ì‹œì‘
    streamer = KafkaDataStreamer(kafka_servers)
    streamer.start_streaming()


if __name__ == "__main__":
    main()
